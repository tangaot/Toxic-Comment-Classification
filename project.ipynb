{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aBrjU9RHlNPe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration\n",
    "---\n",
    "\n",
    "We have create some figure and tables which shown on the presentation and report in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataset_exploration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoHnHQ0Ypl_N"
   },
   "source": [
    "Data Pre-processing\n",
    "----\n",
    "\n",
    "1.Translate Non-English word to English word use Google Translate API\n",
    "\n",
    "We using parallel programming to translate all train data and test data, the approximately  running time is 13 hours for this phase.\n",
    "\n",
    "For example: translate_train_0_to_40000.py translate the train data from first row to 39999th row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run translate_train_0_to_40000.py\n",
    "%run translate_train_40000_to_80000.py\n",
    "%run translate_train_80000_to_120000.py\n",
    "%run translate_train_120000_to_end.py\n",
    "%run translate_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will merge all train data to one file and do the following phase:\n",
    "\n",
    "2.Convert all character to lowercase\n",
    "\n",
    "3.Replace emoticons with word\n",
    "\n",
    "4.Replace Date, Phone Number, and Website Links\n",
    "\n",
    "5.Remove all numbers and punctuations\n",
    "\n",
    "6.Normalize repeating characters\n",
    "\n",
    "7.Remove stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD8j03AosPkR"
   },
   "source": [
    "TF-IDF method\n",
    "---\n",
    "\n",
    "Logistic Regression\n",
    "---\n",
    "\n",
    "Logistic regression was the first model that we constructed. Each class was considered independent in this approach, therefore, we fed as input the converted comments by TF-IDF and use sklearn's default LogisticRegression package trained six models for six classes on training data. The total training time for six models around 57s. Predictions on validation were comparable but in order to solve overfitting, there was still scope for improvement. As Figure 7, we modified the regularization values to tackle overfitting and we find an optimal value that reduces overfitting and holds the accuracy score high enough. A test accuracy of 0.9722 was obtained by doing this.\n",
    "\n",
    "Naive Bayes\n",
    "---\n",
    "Our second model is Naive Bayes. To do this, we fed as input the converted comments by TF-IDF into sklearn's naive\\_bayes package. The approximately training time is  0.6s, this is much faster than Logistic Regression. We tune the alpha parameter using cross validation to find the best performance for each toxicity label. Finally achieved an accuracy score 0.9708 on test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run NaiveBayes.ipynb\n",
    "%run logistic_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_tzQQCvxdKA"
   },
   "source": [
    "Word Embedding method\n",
    "---\n",
    "\n",
    "Word2vec CNN.ipynb uses word2vec embeddings and trains CNN.\n",
    "\n",
    "Glove CNN.ipynb uses glove embeddings and trains CNN. \n",
    "\n",
    "Also, LSTM-RNN.ipynb is use keras word embeddings and trains LSTM. \n",
    "\n",
    "To implement CNN and RNN, we use Google Co-laboratory as a platform to run Jupyter Notebook with GPU support. The following is the link to google drive:\n",
    "\n",
    "https://drive.google.com/drive/folders/1ZTpsQLDzh4kBrI4VFJqKm-uhi_VfbxjG?usp=sharing\n",
    "\n",
    "\n",
    "Word2vec CNN\n",
    "---\n",
    "For the CNN model we varied kernel size, pool size, the number of hidden layers, batch size and epoch. Different optimization algorithms are applied and the best the optimizer is selected by the loss function. As Figure 8, an example of performance vs epoch for different optimiser. We also used validation dataset to early stop the model train, so whenever the validation loss increases we stop training our model.\n",
    "\n",
    "\n",
    "For input with Word2Vec embedding, training time for each epoch is around 12s using Google Co-laboratory with GPU support. We found Adam optimizer with learning rate = 0.001 gives has lowest loss on both training set and validation set. Adam optimization is a method of stochastic gradient descent focused on first-order and second-order moments' adaptive estimation. Finally the accuracy on test dataset is 0.9724.\n",
    "\n",
    "GloVe CNN\n",
    "---\n",
    "For input with GloVe embedding, training time for each epoch is around 8s using Google Co-laboratory with GPU support. We found the Gradient descent optimizer (GSD) with learning rate = 0.01, momentum = 0.9, and = decay 0.0001 has the best performance on both training set and validation set. Finally it achieved 0.9730 accuracy score on test dataset.\n",
    "\n",
    "RNN-LSTM\n",
    "---\n",
    "A recurrent neural network is similar to a CNN in that it has an internal state to process the input sequences. We use LSTM because it preserves the input sequences and assist in understanding the context for better classification. To generate a vector of length 300, we used the default Keras embedding layer.\n",
    "\n",
    "The data is prepared with Keras embedding. The 2-dimensional input is used to train a LSTM model with one hidden layer at 64 output units per layer. It took around 37s to train the model on Google Co-laboratory with GPU support. By incorporating additional layers, which would take more computational power, the model can be further improved. On the test dataset, this model achieved 0.9738 accuracy score, which is the best performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_CNN.ipynb\n",
    "%run glove_CNN.ipynb\n",
    "%run RNN-LSTM.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
