{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "HA3k1n6iZQE3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iDEcTTuZmzr",
    "outputId": "438a62fd-7e02-4f8c-fe3c-6767ede48572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "O4xG3k8IZ27U"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/nlpclass-1207-g-outputerror-master-project/project/processed_data/train_translated_cleaned.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/nlpclass-1207-g-outputerror-master-project/project/processed_data/test_translated_cleaned.csv')\n",
    "test_y = pd.read_csv(\"/content/drive/MyDrive/nlpclass-1207-g-outputerror-master-project/project/processed_data/test_labels_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "QlL-x5-gapNT"
   },
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(lambda x: np.str_(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: np.str_(x))\n",
    "\n",
    "X = train.iloc[:,1]\n",
    "y = train.iloc[:,2:]\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.4, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "C_3XAM3lax30"
   },
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv(\"/content/drive/MyDrive/nlpclass-1207-g-outputerror-master-project/project/original_data/reference_data/bad_words.csv\")\n",
    "bad_words = list(bad_words.bad_words.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UojYXUN3bCWL"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 5000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "num_badwords = len(bad_words)\n",
    "n = 0\n",
    "temp_bw = bad_words\n",
    "for word, i in word_index.items():\n",
    "    if word in bad_words:\n",
    "        temp_bw.remove(word)\n",
    "        n = n+1\n",
    "    if i > (NUM_WORDS-num_badwords+n):\n",
    "        for bw in temp_bw:\n",
    "            tokenizer.word_index[bw] = i\n",
    "            i=i+1\n",
    "        break           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "SLfpas8nbIE9"
   },
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_validation = tokenizer.texts_to_sequences(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "3PFhDyV3bPNc"
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(sequences_train,maxlen=50)\n",
    "X_validation = pad_sequences(sequences_validation,maxlen=X_train.shape[1])\n",
    "\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_validation = np.asarray(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "HgVWwq4xbTz1"
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, embedding_vecor_length, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2k9iGX0bYI8",
    "outputId": "8013d35f-f01b-4da4-96fd-f072b68870f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "240/240 - 13s - loss: 0.0491 - val_loss: 0.0604\n",
      "Epoch 2/20\n",
      "240/240 - 12s - loss: 0.0462 - val_loss: 0.0600\n",
      "Epoch 3/20\n",
      "240/240 - 12s - loss: 0.0433 - val_loss: 0.0655\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt)\n",
    "\n",
    "# Fitting Model to the data\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "hist_adam = model.fit(X_train, y_train, batch_size=400, epochs=20, verbose=2,\n",
    "                      validation_data=(X_validation, y_validation),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "69tgJLBAdXNF"
   },
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(test['comment_text'])\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "pred_val = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "SUAClvFidlgY"
   },
   "outputs": [],
   "source": [
    "for j in range(0,6):\n",
    "    temp = pred_train[:,j]\n",
    "    for i in range(0,len(temp)):\n",
    "        if temp[i] > 0.75:\n",
    "            pred_train[:,j][i] = 1\n",
    "        else: \n",
    "            pred_train[:,j][i] = 0\n",
    "    \n",
    "for j in range(0,6):\n",
    "    temp = pred_test[:,j]\n",
    "    for i in range(0,len(temp)):\n",
    "        if temp[i] > 0.75:\n",
    "            pred_test[:,j][i] = 1\n",
    "        else: \n",
    "            pred_test[:,j][i] = 0\n",
    "            \n",
    "for j in range(0,6):\n",
    "    temp = pred_val[:,j]\n",
    "    for i in range(0,len(temp)):\n",
    "        if temp[i] > 0.75:\n",
    "            pred_val[:,j][i] = 1\n",
    "        else: \n",
    "            pred_val[:,j][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqUIL3NndnaK",
    "outputId": "e565f7df-2683-4c26-c06e-8f00ea163b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic Train Accuracy: 0.9553800839756846 , Val Accuracy: 0.9504927227435805 , Test Accuracy: 0.9377285942042577\n",
      "severe_toxic Train Accuracy: 0.9905266236343506 , Val Accuracy: 0.9900828776888249 , Test Accuracy: 0.9943886961142893\n",
      "obscene Train Accuracy: 0.9777631551461219 , Val Accuracy: 0.9764683764433094 , Test Accuracy: 0.9653005720716497\n",
      "threat Train Accuracy: 0.9970650289319212 , Val Accuracy: 0.9969136286014194 , Test Accuracy: 0.9967019913095126\n",
      "insult Train Accuracy: 0.9673706419335297 , Val Accuracy: 0.9672405959673502 , Test Accuracy: 0.9593922911000656\n",
      "identity_hate Train Accuracy: 0.991247310480249 , Val Accuracy: 0.9912265584608877 , Test Accuracy: 0.9891525211791553\n",
      "Average Train Accuracy: 0.9798921406836429 , Average Val Accuracy: 0.9787374599842287 , Average Test Accuracy: 0.9737774443298216\n"
     ]
    }
   ],
   "source": [
    "col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "Accuracy = np.zeros((3,6))\n",
    "for i,x in enumerate(col):\n",
    "    acc = np.array([metrics.accuracy_score(y_train[:,i], pred_train[:,i]),\n",
    "                    metrics.accuracy_score(y_validation[:,i], pred_val[:,i]),\n",
    "                    metrics.accuracy_score(test_y[x], pred_test[:,i])])\n",
    "    print(x,\"Train Accuracy:\",acc[0],\", Val Accuracy:\",acc[1],\", Test Accuracy:\",acc[2])\n",
    "    Accuracy[:,i] = acc\n",
    "    \n",
    "avg_accuracy = Accuracy.mean(axis=1)\n",
    "print(\"Average Train Accuracy:\",avg_accuracy[0],\n",
    "      \", Average Val Accuracy:\",avg_accuracy[1],\n",
    "      \", Average Test Accuracy:\",avg_accuracy[2])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
